{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple card game IA\n",
    "\n",
    "We will describe in this notebook a very simple 2 players card game and a way to implement an IA to play this game using an **artificial neural network** (we will use a multi layer perceptron model)\n",
    "\n",
    "The IA will be trained only by playing, which means that no \"expert\" human player will implement strategies.\n",
    "\n",
    "The only information about the game given to the IA is a function which indicates to the IA the cards allowed to play.\n",
    "\n",
    "## The game\n",
    "The game is a very basic 2 player \"Trick-taking\" card game :\n",
    "\n",
    "### General description\n",
    "* There are 2 colors (color 0 and color 1)\n",
    "* Each color has N cards (valued from 0 to N-1)\n",
    "* The game is composed of N \"tricks\"\n",
    "* Player 0 starts to play for the first trick\n",
    "* The winner of a trick starts to play the next trick\n",
    "\n",
    "### Trick rules\n",
    "* The first player can play any card\n",
    "* If the second player has cards with the same color, ** he has to play the same color**\n",
    "* Otherwise he plays the other color\n",
    "\n",
    "### Trick winner\n",
    "* If the second player plays the same color. **The card with the highest value wins the trick**\n",
    "* If the second player does not have the same color. **The first player wins**\n",
    "\n",
    "### Example\n",
    "* N = 3\n",
    "* We denote (1,0) card 1 of color 0\n",
    "* Player 0's hand = [(0, 0), (2, 0), (1, 1)]\n",
    "* Player 1's hand = [(1, 0), (0, 1), (2, 1)]\n",
    " * Player 0 plays (2,0)\n",
    " * Player 1 plays (1,0)\n",
    "   * Player 0 wins the trick since 2 > 1 and start to play next trick\n",
    " * Player 0 plays (0,0)\n",
    " * Player 1 plays (0,1)\n",
    "   * Player 0 wins the trick since player 1 with no color 0, played color 1\n",
    " * Player 0 plays (1, 1)\n",
    " * Player 1 plays (2, 1)\n",
    "   * Player 1 wins the trick since 2 > 1\n",
    "* At the end of the game, player 0 wins 2 tricks, player 1 wins 1 trick\n",
    "\n",
    "## Strategies comparison\n",
    "\n",
    "In order to compare strategies, the players will play a twice :\n",
    "* Player 0 with hand 0, Player 1 with hand 1, Player 0 stats\n",
    "* Player 1 with hand 0, Player 0 with hand 1, Player 1 stats\n",
    "\n",
    "Then we compare both games :\n",
    "* If player 0 wins more tricks with hand 0 than player 1 with hand 0, player 0 wins the whole game\n",
    "* If player 1 wins more tricks with hand 0 than player 0 with hand 0, player 1 wins the whole game\n",
    "* Otherwise its draw\n",
    "\n",
    "## IA implementation\n",
    "We will implement a simple MLP model using keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importation\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General functions to generate hands\n",
    "def generateDeck(N=10, P=2):\n",
    "    deck = np.zeros((P*N, 2))\n",
    "    for i in range(0,P):\n",
    "        deck[i * N:i * N + N,0] = np.arange(N)\n",
    "        deck[i * N:i * N + N,1] = i\n",
    "    return deck\n",
    "\n",
    "def gererateHand(N=10, P=2, seed=0):\n",
    "    \"\"\" Generate N cards hands for P players.\n",
    "    The function returns the deck indices\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    deck = np.arange(N*P)\n",
    "    np.random.shuffle(deck)\n",
    "    return np.reshape(deck, (P,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rules\n",
    "Here is the rule function that indicates to the player the cards they can play, according to their hand and the previous trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playable(hand, deck, trick=[]):\n",
    "    \"\"\"Returns indices of playable cards \"\"\"\n",
    "    allrange = np.arange(len(hand))\n",
    "    if len(trick) == 0:\n",
    "        return allrange # First to play\n",
    "\n",
    "    color = deck[trick[0]][1]\n",
    "    colors = allrange[deck[hand][:,1] == color]\n",
    "    if len(colors) > 0: # Forced to play same color\n",
    "        return colors\n",
    "    else: # Does not have the color\n",
    "        return allrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Strategy\n",
    "\n",
    "Here is the **random strategy**.\n",
    "\n",
    "The player plays the first card he founds and since cards are distributed in a random way (see gererateHand()) its equivalent to a random selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def firstPlay(hand, deck, trick=[]):\n",
    "    choices = playable(hand, deck, trick)\n",
    "    return choices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First implementation\n",
    "\n",
    "We will first implement 2 players playing the same random strategy.\n",
    "Since everything is seeded, all the game should end with a **draw** result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playTrick(hands, hist, deck, dealer=0):\n",
    "    \"\"\"Let the players play one trick \"\"\"\n",
    "    tricks = []\n",
    "    P = len(hands)\n",
    "    for player in range(dealer, dealer+P):\n",
    "        player %= P\n",
    "        if player == 0:\n",
    "            card_index = firstPlay(hands[player], deck, tricks) # Player 1 plays \"firstPlay strategy\"\n",
    "        elif player == 1:\n",
    "            card_index = firstPlay(hands[player], deck, tricks) # Player 2 plays \"firstPlay strategy\"\n",
    "        tricks.append(hands[player][card_index])\n",
    "    return tricks\n",
    "\n",
    "\n",
    "def getWinner(trick, deck):\n",
    "    \"\"\"Computes which player wins the trick \"\"\"\n",
    "    good_trump = deck[trick][:,1] == deck[trick[0],1] # players with the right color\n",
    "    winner = 0\n",
    "    for i in range(1, len(trick)):\n",
    "        if good_trump[i] and deck[trick[i]][0] > deck[trick[winner]][0]:\n",
    "            winner = i\n",
    "    return winner\n",
    "\n",
    "\n",
    "def getHands(hands, played_cards):\n",
    "    \"\"\" Extract current hands from played cards\"\"\"\n",
    "    cleanHand = []\n",
    "    for hand in hands:\n",
    "        cleanHand.append(hand[np.array([card not in played_cards for card in hand])])\n",
    "    return cleanHand\n",
    "\n",
    "\n",
    "def play(hands, deck, dealer=0):\n",
    "    \"\"\"Plays N tricks and returns played_card order, first player, and winner\"\"\"\n",
    "    played_cards = []\n",
    "    who_play = []\n",
    "    winner = []\n",
    "    \n",
    "    for i in range(0, len(hands[0])): # N tricks\n",
    "        # P players starting from dealer\n",
    "        who_play += list((np.arange(0, len(hands)) + dealer) % len(hands))\n",
    "\n",
    "        trick = playTrick(getHands(hands, played_cards), played_cards, deck, dealer=dealer)      \n",
    "        played_cards += trick # Save played card history\n",
    "        \n",
    "        winner_rel = getWinner(trick, deck)\n",
    "        dealer = (dealer + winner_rel) % len(hands) # Compute which player will start next\n",
    "        winner.append(dealer)\n",
    "    return (played_cards, winner, who_play)\n",
    "\n",
    "\n",
    "def shiftHand(hands):\n",
    "    \"\"\"Exchange hands so that each player can play each hand\"\"\"\n",
    "    hands2 = np.zeros(hands.shape, dtype=hands.dtype )\n",
    "    hands2[0] = hands[-1]\n",
    "    hands2[1:] = hands[0:-1]\n",
    "    return hands2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets play\n",
    "Now both players can play the first random strategy\n",
    "Lets generate a deck, play several games and compute the winner.\n",
    "According to the implementation, all games must be draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 100.0\n"
     ]
    }
   ],
   "source": [
    "deck0 = generateDeck(N=10, P=2)\n",
    "num_of_games = 1000\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "d = 0\n",
    "\n",
    "for i in range(0, num_of_games):\n",
    "    # Lets play num_of_games games\n",
    "    hand0 = gererateHand(N=10, P=2, seed=i) # seed is fixed\n",
    "\n",
    "    (hist, win, who_play) = play(hand0, deck0, 0)\n",
    "    (shift_hist, shift_win, shift_who_play) = play(shiftHand(hand0), deck0, 1)\n",
    "\n",
    "    if sum([w == 0 for w in win]) > sum([w == 1 for w in shift_win]):\n",
    "        w1 += 1\n",
    "    elif sum([w == 0 for w in win]) < sum([w == 1 for w in shift_win]):\n",
    "        w2 += 1\n",
    "    else:\n",
    "        d += 1\n",
    "print(w1 / num_of_games * 100, w2 / num_of_games * 100, d / num_of_games * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result:\n",
    "\n",
    "0.0 0.0 100.0\n",
    "\n",
    "=> 100% of played games ends with draw (as expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random strategies\n",
    "Lets see how 2 players with random selection (but not the same) plays together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomPlay(hand, deck, trick=[]):\n",
    "    choices = playable(hand, deck, trick)\n",
    "    return np.random.random_integers(0,len(choices) - 1)\n",
    "\n",
    "def playTrick(hands, hist, deck, dealer=0):\n",
    "    \"\"\"Let the players play one trick \"\"\"\n",
    "    tricks = []\n",
    "    P = len(hands)\n",
    "    for player in range(dealer, dealer+P):\n",
    "        player %= P\n",
    "        if player == 0:\n",
    "            card_index = randomPlay(hands[player], deck, tricks) # Player 1 plays \"randomPlay strategy\"\n",
    "        elif player == 1:\n",
    "            card_index = randomPlay(hands[player], deck, tricks) # Player 2 plays \"randomPlay strategy\"\n",
    "        tricks.append(hands[player][card_index])\n",
    "    return tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.8 43.3 14.899999999999999\n"
     ]
    }
   ],
   "source": [
    "deck0 = generateDeck(N=10, P=2)\n",
    "num_of_games = 1000\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "d = 0\n",
    "\n",
    "for i in range(0, num_of_games):\n",
    "    # Lets play num_of_games games\n",
    "    hand0 = gererateHand(N=10, P=2, seed=i) # seed is fixed\n",
    "\n",
    "    (hist, win, who_play) = play(hand0, deck0, 0)\n",
    "    (shift_hist, shift_win, shift_who_play) = play(shiftHand(hand0), deck0, 1)\n",
    "\n",
    "    if sum([w == 0 for w in win]) > sum([w == 1 for w in shift_win]):\n",
    "        w1 += 1\n",
    "    elif sum([w == 0 for w in win]) < sum([w == 1 for w in shift_win]):\n",
    "        w2 += 1\n",
    "    else:\n",
    "        d += 1\n",
    "print(w1 / num_of_games * 100, w2 / num_of_games * 100, d / num_of_games * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results : P1 P2 (100 - P1 - P2) with P1 close to P2\n",
    "Here we have for example:\n",
    "* 44.80 41.6 13.60 with seed=i\n",
    "* 41.80 43.3 14.90 with seed=10*i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "Now we will implement a neural network (without training it) with one hidden layer\n",
    "\n",
    "## Network output \n",
    "We implement one output per card. The choosen card will be the **allowed** card with the highest value\n",
    "\n",
    "## Network input\n",
    "We implement a concatenation of 3 binary inputs :\n",
    "* One vector of size len(deck) representing the cards already played\n",
    "* One vector of size len(deck) representing the current hand\n",
    "* One vector of size len(deck) representing the current trick\n",
    "\n",
    "### Card already played\n",
    "Example with N = 3\n",
    "\n",
    "[0, 0, 1, 1, 0, 0] will represent a game in which card 2 and 3 of the deck have been already played. Which means in the case of 2 colors the card (2, 0) and (0, 1)\n",
    "\n",
    "### Current hand\n",
    "[1, 0, 0, 0, 0, 1] will represent a hand with cards 2 and 3 of the deck, i.e card (0,0) and (2,1)\n",
    "\n",
    "### Trick\n",
    "When player is the first player to play, the trick will be [0, 0, 0, 0, 0, 0]\n",
    "When player is the second player to play, the trick represents the played card.\n",
    "Example [0, 1, 0, 0, 0, 0] means that other player plays card (1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_nn(in_, hidden_, out_):\n",
    "    \"\"\"Creates the neural network\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(in_, hidden_, init='uniform', activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(hidden_, out_, init='uniform', activation='softmax'))\n",
    "\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rms)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets implement the NN playing strategy. We add some function allowing us convert a card to its binary array representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def card2vec(card_value, card_color, N=10, P=2):\n",
    "    \"\"\"Convert a card from its (value, color) representation to its binary array representation\"\"\"\n",
    "    vec = np.zeros(N * P)\n",
    "    vec[card_color * N + card_value] = 1\n",
    "    return vec\n",
    "\n",
    "def vec2card(vec, N=10):\n",
    "    \"\"\"Convert a card from its binary array representation to its (value, color) representation\"\"\"\n",
    "    index = np.argwhere(vec == 1)[0][0]\n",
    "    return (index % N, int(index / N))\n",
    "\n",
    "def hist2vec(hist, deck, N=10, P=2):\n",
    "    \"\"\"Convert a list of card\"\"\"\n",
    "    hist_vec = np.zeros((len(hist), len(deck)))\n",
    "    for i in range(0, len(hist)):\n",
    "        vec = card2vec(deck[hist[i]][0], deck[hist[i]][1], N=N, P=P)\n",
    "        hist_vec[i] = vec\n",
    "    return hist_vec\n",
    "\n",
    "def nnPlay(mod, hand, hist, deck, trick=[]):\n",
    "    choices = playable(hand, deck, trick)\n",
    "\n",
    "    hist_cards = np.sum(hist2vec(hist, deck0, 10, 2), axis=0)\n",
    "    \n",
    "    current_hand_vec = np.zeros(len(deck0))\n",
    "    for j in hand:\n",
    "        current_hand_vec[j] = 1\n",
    "\n",
    "    if trick:\n",
    "        new_X = np.concatenate([\n",
    "            hist_cards,\n",
    "            current_hand_vec,\n",
    "            hist2vec(trick, deck)[0]\n",
    "        ])\n",
    "    else:\n",
    "        new_X = np.concatenate([\n",
    "            hist_cards,\n",
    "            current_hand_vec,\n",
    "            np.zeros(20)\n",
    "        ])\n",
    "\n",
    "    probas = mod.predict(new_X.reshape((1,-1)), verbose=0)\n",
    "    return choices[np.argmax(probas[0][hand[choices]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untrained NN\n",
    "Lets try our untrained neural network agaist a firstPlay strategy\n",
    "We use 2 * len(deck) neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modl = compile_nn(3 * len(deck0),  2 * len(deck0), len(deck0))\n",
    "# We keep modl has a global variable in order to only change interesing functions\n",
    "\n",
    "def playTrick(hands, hist, deck, dealer=0):\n",
    "    tricks = []\n",
    "    P = len(hands)\n",
    "    for player in range(dealer, dealer+P):\n",
    "        player %= P\n",
    "        if player == 0:\n",
    "            card_index = nnPlay(modl, hands[player], hist, deck, tricks)\n",
    "        elif player == 1:\n",
    "            card_index = firstPlay(hands[player], deck, tricks)\n",
    "        tricks.append(hands[player][card_index])\n",
    "    return tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.4 37.6 24.0\n"
     ]
    }
   ],
   "source": [
    "deck0 = generateDeck(N=10, P=2)\n",
    "num_of_games = 1000\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "d = 0\n",
    "\n",
    "for i in range(0, num_of_games):\n",
    "    # Lets play num_of_games games\n",
    "    hand0 = gererateHand(N=10, P=2, seed=10 * i) # seed is fixed\n",
    "\n",
    "    (hist, win, who_play) = play(hand0, deck0, 0)\n",
    "    (shift_hist, shift_win, shift_who_play) = play(shiftHand(hand0), deck0, 1)\n",
    "\n",
    "    if sum([w == 0 for w in win]) > sum([w == 1 for w in shift_win]):\n",
    "        w1 += 1\n",
    "    elif sum([w == 0 for w in win]) < sum([w == 1 for w in shift_win]):\n",
    "        w2 += 1\n",
    "    else:\n",
    "        d += 1\n",
    "print(w1 / num_of_games * 100, w2 / num_of_games * 100, d / num_of_games * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results : P1 P2 (100 - P1 - P2) with P1 close to P2\n",
    "Here we have for example:\n",
    "    \n",
    "* 37.8 39.2 23.0 with seed=i\n",
    "* 38.4 37.6 24.0 with seed=10*i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets train the Network !\n",
    "\n",
    "## Good games\n",
    "Firsly we will train the network by feeding it with cards played when he wins the game\n",
    "\n",
    "For each won game, we feed the network as if **each card** he plays during this game was **the card** to play, i.e. was the expected label.\n",
    "\n",
    "We catch \"good\" examples with learn_win() functions called when the player wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_win(hist, X, y):\n",
    "    hist_vec = hist2vec(hist, deck0, 10, 2)\n",
    "    hist_cards = np.zeros(20)\n",
    "    current_hand = hand0[0]\n",
    "    \n",
    "    for i in range(0, int(len(hist_vec) / 2)): # For each trick\n",
    "        \n",
    "        # Re-construct the current hand of this particular trick\n",
    "        current_hand_vec = np.zeros(len(deck0))\n",
    "        for j in current_hand:\n",
    "            current_hand_vec[j] = 1\n",
    "        \n",
    "        # 2 cases : Player plays first or second\n",
    "        if (who_play[2 * i + 1] == 0): # Win when 2nd to play\n",
    "            y.append(hist_vec[2 * i + 1]) # It was THE card to play !\n",
    "            new_X = np.concatenate([\n",
    "                hist_cards,\n",
    "                current_hand_vec,\n",
    "                hist_vec[2 * i]\n",
    "            ])\n",
    "            X.append(new_X)\n",
    "            current_hand = np.delete(current_hand, np.where(current_hand == hist[2 * i + 1])[0][0])\n",
    "\n",
    "        elif (who_play[2 * i] == 0): # Win when 1st to play\n",
    "            y.append(hist_vec[2 * i]) # It was THE card to play !\n",
    "            new_X = np.concatenate([\n",
    "                hist_cards,\n",
    "                current_hand_vec,\n",
    "                np.zeros(20)\n",
    "            ])\n",
    "            X.append(new_X)\n",
    "            current_hand = np.delete(current_hand, np.where(current_hand == hist[2 * i])[0][0])\n",
    "        hist_cards += hist_vec[2 * i] + hist_vec[2 * i + 1]       \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will play several batches of **different** num_of_games. So that the network can learn from each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0 37.8 27.200000000000003\n",
      "40.699999999999996 33.300000000000004 26.0\n",
      "44.800000000000004 28.199999999999996 27.0\n",
      "45.9 31.1 23.0\n",
      "51.6 25.900000000000002 22.5\n",
      "48.199999999999996 26.5 25.3\n",
      "48.9 26.3 24.8\n",
      "52.800000000000004 23.7 23.5\n",
      "51.5 24.099999999999998 24.4\n",
      "46.400000000000006 27.400000000000002 26.200000000000003\n"
     ]
    }
   ],
   "source": [
    "modl = compile_nn(3 * len(deck0),  2 * len(deck0), len(deck0))\n",
    "\n",
    "deck0 = generateDeck(N=10, P=2)\n",
    "num_of_games = 1000\n",
    "\n",
    "for j in range(0, 10): # LETS PLAY 10 TIMES !\n",
    "    w1 = 0\n",
    "    w2 = 0\n",
    "    d = 0\n",
    "\n",
    "    # Examples used to train the network\n",
    "    # We accumulate examples during all the num_of_games games\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(0, num_of_games):\n",
    "        hand0 = gererateHand(N=10, P=2, seed=i + j * num_of_games) # seed is always differebt\n",
    "\n",
    "        (hist, win, who_play) = play(hand0, deck0, 0)\n",
    "        (shift_hist, shift_win, shift_who_play) = play(shiftHand(hand0), deck0, 1)\n",
    "\n",
    "        if sum([w == 0 for w in win]) > sum([w == 1 for w in shift_win]):\n",
    "            # HERE WE WIN\n",
    "            # So we use the information to train the network\n",
    "            (X, y) = learn_win(hist, X, y)\n",
    "            w1 += 1\n",
    "        elif sum([w == 0 for w in win]) < sum([w == 1 for w in shift_win]):\n",
    "            w2 += 1\n",
    "        else:\n",
    "            d += 1\n",
    "    print(w1 / num_of_games * 100, w2 / num_of_games * 100, d / num_of_games * 100)\n",
    "    \n",
    "    # Now we have good examples, we can train the network for the next time !\n",
    "    modl.fit(np.array(X), np.array(y), nb_epoch=20, verbose=0)\n",
    "\n",
    "mod0 = modl # Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results :\n",
    "We expect our network to be quite better than the random wich means that P1 must increase and be better than P2\n",
    "\n",
    "Example of generated results :\n",
    "* 35.5 37.6 26.90\n",
    "* 40.1 33.6 26.3\n",
    "* 40.5 36.20 23.3\n",
    "* 41.70 33.30 25.0\n",
    "* 45.2 27.50 27.3\n",
    "* 47.4 26.8 25.8\n",
    "* 45.1 30.0 24.9\n",
    "* 48.4 23.1 28.50\n",
    "* 48.9 27.80 23.3\n",
    "* 50.1 24.7 25.2\n",
    "\n",
    "Ok seems to work since NN player wins 50% of the time whereas random strategy wins only 25% !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn when loosing\n",
    "With the first model, the network learn only when it wins. We can try to train it when it loose, by teaching it not to play the card it plays.\n",
    "\n",
    "We feed the labels we a zero probabiliy for the played cards and feed other **allowed** card with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_loose(hist, X, y):\n",
    "    hist_vec = hist2vec(hist, deck0, 10, 2)\n",
    "    hist_cards = np.zeros(20)\n",
    "    current_hand = hand0[0]\n",
    "    \n",
    "    for i in range(0, int(len(hist_vec) / 2)):\n",
    "        current_hand_vec = np.zeros(len(deck0))\n",
    "        for j in current_hand:\n",
    "            current_hand_vec[j] = 1\n",
    "\n",
    "        if (who_play[2 * i + 1] == 0): # Loose when 2nd to play           \n",
    "            ye = np.zeros(20)\n",
    "            ye[current_hand] = 1\n",
    "            ye -= hist_vec[2 * i + 1]            \n",
    "            y.append(ye) # Maybe I could choose another card of my hand\n",
    "            \n",
    "            new_X = np.concatenate([\n",
    "                hist_cards,\n",
    "                current_hand_vec,\n",
    "                hist_vec[2 * i]\n",
    "            ])\n",
    "            X.append(new_X)\n",
    "            current_hand = np.delete(current_hand, np.where(current_hand == hist[2 * i + 1])[0][0])\n",
    "        elif (who_play[2 * i] == 0): # Loose when 1st to play\n",
    "\n",
    "            ye = np.zeros(20)\n",
    "            ye[current_hand] = 1\n",
    "            ye -= hist_vec[2 * i]            \n",
    "            y.append(ye) # Maybe I could choose another card of my hand\n",
    "\n",
    "            new_X = np.concatenate([\n",
    "                hist_cards,\n",
    "                current_hand_vec,\n",
    "                np.zeros(20)\n",
    "            ])\n",
    "            X.append(new_X)\n",
    "            current_hand = np.delete(current_hand, np.where(current_hand == hist[2 * i])[0][0])\n",
    "        hist_cards += hist_vec[2 * i] + hist_vec[2 * i + 1]       \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train it each time !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0 71.0 16.0\n",
      "11.5 72.5 16.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-8c7f54da2f4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwho_play\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhand0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mshift_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift_win\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift_who_play\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshiftHand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhand0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwin\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshift_win\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-813b9d854d72>\u001b[0m in \u001b[0;36mplay\u001b[1;34m(hands, deck, dealer)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mplayed_cards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrick\u001b[0m \u001b[1;31m# Save played card history\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mwinner_rel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetWinner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrick\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mdealer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdealer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwinner_rel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhands\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Compute which player will start next\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mwinner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdealer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-813b9d854d72>\u001b[0m in \u001b[0;36mgetWinner\u001b[1;34m(trick, deck)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetWinner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrick\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;34m\"\"\"Computes which player wins the trick \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mgood_trump\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrick\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrick\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# players with the right color\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mwinner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrick\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modl = compile_nn(3 * len(deck0),  2 * len(deck0), len(deck0))\n",
    "\n",
    "deck0 = generateDeck(N=10, P=2)\n",
    "num_of_games = 1000\n",
    "\n",
    "for j in range(0, 10): # LETS PLAY 10 TIMES !\n",
    "    w1 = 0\n",
    "    w2 = 0\n",
    "    d = 0\n",
    "\n",
    "    # Examples used to train the network\n",
    "    # We accumulate examples during all the num_of_games games\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(0, num_of_games):\n",
    "        hand0 = gererateHand(N=10, P=2, seed=i + j * num_of_games) # seed is always differebt\n",
    "\n",
    "        (hist, win, who_play) = play(hand0, deck0, 0)\n",
    "        (shift_hist, shift_win, shift_who_play) = play(shiftHand(hand0), deck0, 1)\n",
    "\n",
    "        if sum([w == 0 for w in win]) > sum([w == 1 for w in shift_win]):\n",
    "            # HERE WE WIN\n",
    "            # So we use the information to train the network\n",
    "            (X, y) = learn_win(hist, X, y)\n",
    "            w1 += 1\n",
    "        elif sum([w == 0 for w in win]) < sum([w == 1 for w in shift_win]):\n",
    "            # HERE WE LOOSE\n",
    "            # So we use the information to train the network\n",
    "            (X, y) = learn_loose(hist, X, y)\n",
    "            w2 += 1\n",
    "        else:\n",
    "            d += 1\n",
    "    print(w1 / num_of_games * 100, w2 / num_of_games * 100, d / num_of_games * 100)\n",
    "    \n",
    "    # Now we have good examples, we can train the network for the next time !\n",
    "    modl.fit(np.array(X), np.array(y), nb_epoch=20, verbose=0)\n",
    "\n",
    "mod2 = modl # Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of generated results :\n",
    "* 35.5 38.7 25.8\n",
    "* 44.7 32.9 22.40\n",
    "* 58.5 18.3 23.20\n",
    "* 67.60 12.0 20.4\n",
    "* 69.5 12.4 18.10\n",
    "* 69.5 12.6 17.9\n",
    "* 69.1 13.5 17.4\n",
    "* 70.3 11.4 18.3\n",
    "* 73.7 10.4 15.9\n",
    "* 72.3 11.4 16.3\n",
    "\n",
    "Well our network now wins 72% of the time vs 12 % for the random strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1 VS NN2\n",
    "Lets now fight our 2 models together !\n",
    "We put very large seeds to be sure both NN have not learn from training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playTrick(hands, hist, deck, dealer=0):\n",
    "    tricks = []\n",
    "    P = len(hands)\n",
    "    for player in range(dealer, dealer+P):\n",
    "        player %= P\n",
    "        if player == 0:\n",
    "            card_index = nnPlay(mod0, hands[player], hist, deck, tricks)\n",
    "        elif player == 1:\n",
    "            # card_index = firstPlay(hands[player], deck, tricks)\n",
    "            card_index = nnPlay(mod1, hands[player], hist, deck, tricks)\n",
    "        tricks.append(hands[player][card_index])\n",
    "    return tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 73.4 15.9\n"
     ]
    }
   ],
   "source": [
    "deck0 = generateDeck(N=10, P=2)\n",
    "num_of_games = 1000\n",
    "w1 = 0\n",
    "w2 = 0\n",
    "d = 0\n",
    "\n",
    "for i in range(0, num_of_games):\n",
    "    # Lets play num_of_games games\n",
    "    hand0 = gererateHand(N=10, P=2, seed=10000 * i) # seed is fixed\n",
    "\n",
    "    (hist, win, who_play) = play(hand0, deck0, 0)\n",
    "    (shift_hist, shift_win, shift_who_play) = play(shiftHand(hand0), deck0, 1)\n",
    "\n",
    "    if sum([w == 0 for w in win]) > sum([w == 1 for w in shift_win]):\n",
    "        w1 += 1\n",
    "    elif sum([w == 0 for w in win]) < sum([w == 1 for w in shift_win]):\n",
    "        w2 += 1\n",
    "    else:\n",
    "        d += 1\n",
    "print(w1 / num_of_games * 100, w2 / num_of_games * 100, d / num_of_games * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of generated results :\n",
    "* 10.7 73.4 15.9\n",
    "\n",
    "The second neural network seem way better !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
